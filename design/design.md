<div>

# üìÑ **Resume Link Tracker ‚Äì System Architecture Overview**

This document explains the full workflow of the Resume Link Tracker system.  
It follows a **decoupled, event-driven architecture** using AWS S3, SQS, Lambda/EC2, PostgreSQL, and Spring Boot.

---

# üî∑ **High-Level Pipeline**

The overall process:

1. User uploads a resume PDF  
2. PDF is stored in S3  
3. Metadata saved to PostgreSQL  
4. PDF Extractor Worker extracts hyperlinks  
5. Core Logic Worker applies mapping rules  
6. Final enriched data is stored in PostgreSQL  

The system is broken into three isolated, scalable services:

- **Spring Upload Service**  
- **PDF Extractor Worker**  
- **Core Logic Worker**

---

# üß© **1. Spring Upload Service**

The upload service is the system's entry point.

### **Responsibilities**
- Receive PDF upload via HTTP POST  
- Upload file to **AWS S3**  
- Create metadata entry in **PostgreSQL**  
- Publish a message to an **SQS queue** for async processing  

### **Workflow**
1. **User ‚Üí Spring Upload API**  
   The user uploads `resume.pdf`.

2. **Upload to S3**  
   Stores the PDF in a private S3 bucket.

3. **Save DB Metadata**  
   The service stores:
   - `resume_id`  
   - `s3_key`  
   - created_at  
   - status  

4. **Publish to SQS (Upload Queue)**  
   Sample published message:

   ```json
   {
     "resume_id": "uuid",
     "s3_key": "uploads/resume_123.pdf"
   }
   ```

   üß© 2. PDF Extractor Worker

This worker processes the resume and extracts URLs.

Responsibilities
	‚Ä¢	Consume SQS messages
	‚Ä¢	Download PDF from S3
	‚Ä¢	Extract hyperlinks inside the PDF
	‚Ä¢	Save extracted data into PostgreSQL
	‚Ä¢	Publish a new SQS event for the Core Logic Worker

Workflow
	1.	Consume SQS Message
Receives the event from the Upload Service.
	2.	Download PDF from S3
	3.	Extract URLs
The worker identifies:
	‚Ä¢	LinkedIn
	‚Ä¢	GitHub
	‚Ä¢	Personal website
	‚Ä¢	Email links
	‚Ä¢	Any clickable hyperlink in the PDF
	4.	Write Extracted URLs into PostgreSQL
	5.	Publish Resume ID to Next SQS Queue
Triggers the Core Logic Worker for further processing.

‚∏ª

üß© 3. Core Logic Worker

This component applies the system‚Äôs business rules.

Responsibilities
	‚Ä¢	Listen to SQS events from PDF Extractor
	‚Ä¢	Retrieve all extracted URLs from PostgreSQL
	‚Ä¢	Apply mapping logic or classification rules
	‚Ä¢	Save the enriched results into the database

Workflow
	1.	Receive resume_id from SQS
	2.	Fetch all extracted URLs from DB
	3.	Apply Mapping Logic
Examples:
	‚Ä¢	Group URLs (social, work, portfolio)
	‚Ä¢	Normalize website variations
	‚Ä¢	Detect duplicates
	‚Ä¢	Apply analytics or scoring rules
	4.	Save Final Processed Output into PostgreSQL

‚∏ª

üèó Tech Stack

Backend
	‚Ä¢	Spring Boot
	‚Ä¢	Spring Security
	‚Ä¢	Spring Data JPA

AWS Cloud
	‚Ä¢	S3 (PDF storage)
	‚Ä¢	SQS (message queues)
	‚Ä¢	Lambda or EC2 (workers)
	‚Ä¢	SNS (optional for fanout patterns)

Database
	‚Ä¢	PostgreSQL

‚∏ª

‚úîÔ∏è Why This Architecture Works
	‚Ä¢	Asynchronous ‚Üí user uploads never wait for processing
	‚Ä¢	Decoupled ‚Üí each stage can scale independently
	‚Ä¢	Fault-tolerant ‚Üí SQS retries prevent processing loss
	‚Ä¢	Modular ‚Üí add new workers easily (analytics, AI, etc.)
	‚Ä¢	Cloud-native ‚Üí uses AWS best practices

‚∏ª

üìå Summary Diagram Explanation
	‚Ä¢	User uploads a resume to Spring Upload Service
	‚Ä¢	Service uploads to S3, writes metadata to PostgreSQL, and publishes message to SQS
	‚Ä¢	PDF Extractor Worker downloads PDF, extracts URLs, saves them, and publishes another event
	‚Ä¢	Core Logic Worker applies mappings and saves final results

This creates a clean, reliable, and scalable resume-processing pipeline.
</div>
```
